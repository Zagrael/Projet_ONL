\documentclass[12pt, a4paper]{report}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{subsection}}
\newcommand{\Lagr}{\mathcal{L}}
\usepackage[a4paper,left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}
\usepackage[fpms]{umons-coverpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{multirow}
\umonsAuthor{Sabrine \textsc{Riahi}\\Aymerick \textsc{Soyez}}
\usepackage[utf8]{inputenc}
\umonsTitle{Optimisation Non Linéaire}
\umonsSubtitle {\\ \textbf{Récupération d’une image floutée (deblurring)}}
\umonsSupervisor {Sous la direction de Monsieur Nicolas \textsc{Gillis} et Arnaud \textsc{Vandaele}}
\umonsDate {Décembre 2017}
\umonsDocumentType {Projet d'Optimisation}

\begin{document}
\umonsCoverPage
\tableofcontents
\clearpage
\section{Introduction}
Le problème posé est de déflouter une image dont chaque pixel a été remplacé par une combinaison linéaire des pixels voisins. La matrice de floutage utilisée est donnée. De plus, un bruit gaussien a été ajouté dans l'image.\\
L'objectif de ce projet est donc la résolution du problème suivant :\\
\[\underset{0 \leq x \leq 1}{\mathrm{min}} ||Ax - \tilde{x}||_2^2 + \lambda||x||_2^2\]
où :\\
$A$ est la matrice de floutage\\
$\tilde{x}$ est le vecteur de pixels flouté\\
$\lambda$ est un paramètre positif qui dépend du niveau de bruit\\

\section{Étude de la convexité du problème}
\noindent
Pour qu'un problème soit convexe, il faut que
\begin{itemize}
\item Le problème soit sous forme de minimisation,
\item Son domaine $ D $ soit convexe,
\item $\forall x,y \in D,\ \forall \lambda \in \left[0;1\right],\ f(\lambda x + (1 - \lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)$
\end{itemize}
\noindent
\linebreak
Le domaine est décrit par \( D = \left\{ x\ |\ c(x) \geq 0 \right\} \) et est convexe si $c(x)$ est concave.\\
Ici : \( D = \left\{ x\ |\ 0 \leq x \leq 1 \right\}\)\\
On a donc deux inégalités $x \geq 0$ et $1 - x \geq 0$. Ces deux fonctions étant linéaires, elles sont à la fois concaves et convexes, donc ces deux \og sous-domaines\fg\ sont convexes.\\
Puisque l'intersection de deux ensembles convexes est un ensemble convexe, l'ensemble $D$ est également convexe.\\

\noindent
La norme 2 est une fonction convexe.\\

\noindent
\textbf{Preuve}\\
En utilisant les propriétés de la norme euclidienne :\\
$\begin{array}{rcl}
f(\lambda x + (1-\lambda)y) & = & ||\ \lambda x + (1-\lambda)y\ ||\\
 & \leq & \lambda\ ||x|| + (1-\lambda)\ ||y||\\
 & = & \lambda f(x) + (1-\lambda)f(y)\\
\end{array}$

\noindent
\newline
On a donc bien une fonction convexe, et on peut conclure que le problème est convexe.\\


\section{Le problème admet-il un minimum global ?}
Le domaine du problème est compact (fermé et borné). De plus, la fonction est convexe sur ce domaine. On aura donc soit un ou plusieurs minima locaux à l'intérieur du domaine, soit un minimum local sur l'une de ses bornes.\\
Le problème étant convexe, tout minimum local est global.\\
On conclut ainsi que le problème admet un minimum global.\\

\section{Conditions d'optimalité}
\subsection*{Conditions d'optimalité de Karush-Kuhn-Tucker}

Si $x^*$ est une solution optimale locale telle que l'ensemble des gradients des contraintes actives est linéairement indépendant, alors il existe un vecteur de multiplicateurs de Lagrange $\alpha^*$
tel que :
\begin{center}
$\begin{array}{rcl}
\nabla_x \Lagr(x^*,\alpha^*)  & = & 0 \\
\alpha^*_i & \geq & 0 \\
\alpha^*_i c_i (x^*) & = & 0 \\
\end{array}$
\end{center}
Nous disposons du problème de minimisation suivant : \\
\[\underset{0 \leq x \leq 1}{\mathrm{min}} ||Ax - \tilde{x}||_2^2 + \lambda||x||_2^2\]\\
En reformulant les contraintes comme suit : 
\begin{center}
$\left\{\begin{array}{rcl}
x_i & \geq & 0 \\
-x_i + 1 & \geq & 0 \\
\end{array}\right.$
\end{center}
Et en tenant compte de la définition de la fonction Lagrangienne :
\begin{center}
$\Lagr(x,\alpha) = f(x) - \sum \alpha_i c_i(x)$ 
\end{center}
On obtient le Lagrangien de notre problème d'optimisation : 
\begin{center}
$\Lagr(x_i, \alpha_i) = ||Ax_i - \tilde{x_i}||_2^2  +  \lambda_i ||x_i||_2^2 - \sum \alpha_i x_i - \sum \alpha_i (- x_i + 1)$
\end{center}
En tenant compte des conditions KKT, nous avons finalement : 

\begin{center}
$\begin{array}{rclcrll}
\nabla_x \Lagr(x_i, \alpha_i) & = & 0 & \Leftrightarrow & \nabla(||Ax_i - \tilde{x_i}||_2^2  +  \lambda_i ||x_i||_2^2 - \sum \alpha_i x_i - \sum \alpha_i (- x_i + 1)) & = & 0\\
\end{array}$
\end{center}

\begin{center}
$\begin{array}{rclcrll}
\alpha_i & \geq & 0 & \Leftrightarrow & \alpha_i & \geq & 0 \\
\end{array}$
\end{center}

\begin{center}
$\begin{array}{rclcrll}
\alpha_i c_i(x_i) & = & 0 & \Leftrightarrow & \left\{\begin{array}{l} \alpha_i x_i \\ \alpha_i(- x_i +1)\end{array}\right. & \begin{array}{c} = \\ = \end{array} & \begin{array}{l} 0 \\ 0 \end{array}\\
\end{array}$
\end{center}

\begin{center}
$\begin{array}{rcl}
x_i & \geq & 0 \\
-x_i + 1 & \geq & 0 \\
\end{array}$
\end{center}
\noindent
Pour que les conditions d'optimalité de KKT soient nécessaires et suffisantes, il faut que le problème soit convexe, et qu'il admette un point de Slater.\\
Il a été démontré plus haut que le problème étudié est convexe. Étant donné qu'il nous est également possible de trouver un point de Slater respectant les contraintes du problème, les conditions d'optimalité de KKT sont donc nécessaires et suffisantes.
 
\section{Méthode de descente de coordonnées}
La méthode de descente de coordonnées consiste à minimiser la fonction successivement sur chaque variable séparément en gardant les autres variables fixées.\\
\\Pour cela, on choisit un point de départ, qui sera, dans le cadre de ce projet, l'image floutée.\\
On calcule ensuite, pour chaque variable, la dérivée par rapport à celle-ci, que l'on égalise à zéro. La valeur obtenue remplacera la précédente pour les prochaines itérations. \\
\\On effectue ces opérations jusqu'à obtenir une convergence des valeurs obtenues pour chaque variable, c'est à dire jusqu'à ce que l'écart entre deux itérations soit inférieur à une précision fixée. \\
\\La fonction que nous devons minimiser est quadratique. Mettons-la sous la forme \[f(x) = \dfrac{1}{2}x^TQx + c^Tx + b\]

\begin{tabular}{rcl}
$||Ax - \tilde{x}||_2^2 + \lambda||x||_2^2$ & = & $(Ax - \tilde{x})^T(Ax - \tilde{x}) + \lambda x^Tx$ \\
 & = & $(Ax)^T(Ax) - \tilde{x}^T(Ax) - (Ax)^T\tilde{x} + \tilde{x}^T\tilde{x} + \lambda x^Tx$ \\
 & = & $x^TA^TAx - 2\tilde{x}^TAx + \tilde{x}^T\tilde{x} + x^T(\lambda I)x$ \\
 & = & $x^T(A^TA + \lambda I)x - 2 \tilde{x}^TAx + \tilde{x}^T\tilde{x}$ \\
\end{tabular}
\newline
\newline
\\
On retrouve bien la forme ci-dessus, avec
$\left\{\begin{array}{rcl}
Q & = & 2(A^TA + \lambda I) \\
c & = & -2A^T\tilde{x} \\
b & = & \tilde{x}^T\tilde{x} \\
\end{array}\right.$
\bigbreak
\noindent
Pour calculer la formule de l'actualisation d'une variable, considérons une fonction quadratique en 3 dimensions : 
\[f(x_1,x_2,x_3) = \dfrac{1}{2}
\left(\begin{array}{ccc}
x_1 & x_2 & x_3
\end{array}\right)
\left(\begin{array}{ccc}
q_{11} & q_{12} & q_{13} \\
q_{21} & q_{22} & q_{23} \\
q_{31} & q_{32} & q_{33}
\end{array}\right)
\left(\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}\right)
+ \left(\begin{array}{ccc}
c_1 & c_2 & c_3
\end{array}\right)
\left(\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}\right)\]
\noindent
Si on veut actualiser $x_2$, on calcule la dérivée de $f$ en cette variable :
\[\dfrac{\delta f(x_1,x_2,x_3)}{\delta x_2} = q_{21}x_1 + q_{22}x_2 + q_{23}x_3 + c_2\]
La nouvelle valeur de $x_2$ est donc donnée par :
\(x_2 = \dfrac{-(q_{21}x_1 + q_{23}x_3 + c_2)}{q_{22}}\) \\
Pour une fonction à $n$ dimensions, en la variable $x_i$ :\\
\begin{center}
\begin{tabular}{rcl}
$x_i$ & = & $\dfrac{-\left(\sum\limits_{\underset{j \neq i}{j = 1}}^n q_{ij}x_j + c_i\right)}{q_{ii}}$ \\
 & = & $x_i - \dfrac{\sum\limits_{j = 1}^nq_{ij}x_j + c_i}{q_{ii}}$ \\
 & = & $x_i - \dfrac{\left[\nabla f\right]_i}{q_{ii}}$ \\
\end{tabular}
\end{center}

Pour respecter la contrainte $0 \leq x \leq 1$, il suffit d'utiliser \(\text{min}(\text{max}(..., 0), 1)\), où "..." est la formule d'actualisation ci-dessus. \\

Nous avons également utilisé un test d'arrêt qui prend en compte la norme de l'écart entre les deux dernières itérations ainsi que le nombre d'itérations. \\
L'implémentation sous Matlab se trouve en annexes.

\section{Méthode du gradient}
La méthode du gradient est similaire à la méthode de descente de coordonnées, en prenant comme direction de descente la direction de plus grande pente.\\
Afin de s'assurer que l'objectif diminue, on détermine un pas optimal pour chaque itération.\\
\\Une itération sera donnée par :
\begin{center}
$X_{k+1}=X_k - \alpha \nabla f(X_k)$ (1)
\end{center}
Pour résoudre le problème, nous le reformulerons sous la forme quadratique, tout comme pour la méthode du gradient, ce qui nous donnera également :
\begin{center}
$f(x) = \dfrac{1}{2}x^TQx + c^Tx + b$ (2)
\end{center}
Avec :
\begin{flushleft}
$\begin{array}{rcl}
Q & = & 2(A^TA + \lambda I) \\
c & = & -2A^T\tilde{x} \\
b & = & \tilde{x}^T\tilde{x} \\
\end{array}$
\end{flushleft}
Le gradient de la fonction est donc égal à :
\begin{center}
$\nabla f(x) = Qx + c$ (3)
\end{center}
Déterminer un pas optimal $\alpha$ pour obtenir une itération $k + 1$ revient donc à résoudre l'équation :
\begin{center}
$\dfrac{\partial f(X_{k+1})}{\partial \alpha} = 0$
\end{center}
En considérant les équations (1), (2) et (3), et en posant$\nabla f(X_k) = g$ , nous pouvons déterminer le pas optimal $\alpha$ pour chaque itération :
\begin{center}
$\begin{array}{rl}
f(X_{k+1}) & = \dfrac{1}{2}X_{k+1}^TQX_{k+1} + c^TX_{k+1} + b\\
\\& = \dfrac{1}{2}(X_k - \alpha g)^TQ(X_k - \alpha g) + c^T(X_k - \alpha g) + b\\
\\& = \dfrac{1}{2}(X_k^T - \alpha g^T)Q(X_k - \alpha g) + c^T(X_k - \alpha g) + b\\
\\& = \dfrac{1}{2}(X_k^T Q X_k- \alpha X_k^T Q g - \alpha g^T Q X_k + \alpha^2g^T Q g) + c^T X_k - \alpha c^T g + b\\
\\& = \dfrac{1}{2}X_k^T Q X_k- \alpha X_k^T Q g +\dfrac{1}{2}\alpha^2g^T Q g + c^T X_k - \alpha c^T g + b
\end{array}$
\bigbreak
\end{center}
En prenant $\dfrac{\partial f(X_{k+1})}{\partial \alpha} = 0$, on obtient :
\begin{center}
$ - Q X_k^T g + \alpha g^TQ g - c^T g = 0$\\
\bigbreak
$\alpha = \dfrac{c^T g + Q X_k^T g}{g^TQ g} = \dfrac{(c^T + Q X_k^T)g}{g^TQ g} = \dfrac{g^T g}{g^TQ g}$\\
\end{center}
\bigbreak
\noindent
L'implémentation sous Matlab se trouve en annexe.
\section{Comparaison des méthodes}
Dans le tableau suivant reprenant l'ensemble des erreurs pour diverses valeurs de $\lambda$ (limité à 3000 itérations), on peut voir que la méthode du gradient est plus efficace pour l'Example1, tandis que la méthode de descente de coordonnées est plus efficace pour la première et la dernière image. \\

Nous avons également observé les différences au niveau du temps de calcul et du nombre d'itérations pour chaque méthode : (limité à 100 000 itérations, sauf la nuit) \\
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Méthode & Example & Nombre d'itérations & Temps de calcul & Erreur \\
\hline
\multirow{3}{*}{Gradient} & 0 & 1 000 000 & 20m & 0.93 \% \\
 & 1 & 100 000 & 45m & 11.80 \% \\
 & 2 & 212 924 & ? & 1.05 \% \\
\hline
\multirow{3}{*}{Descente de coordonnées} & 0 & 1 159 & < 1m & 0.93 \% \\
 & 1 & 44 314 & 4h & 14.00 \% \\
 & 2 & 2 900 & 20m & 1.05 \% \\
\hline
\end{tabular}
\end{center}

La méthode de descente de coordonnées nécessite moins d'itérations, mais il faut plus de temps pour les exécuter. \\

%La méthode de descente de coordonnées est en générale plus efficace (converge plus rapidement) pour des fonctions dont on peut séparer les différentes variables : $f(x_1,x_2) = g(x_1) + h(x_2)$, ce qui est le cas d'une fonction quadratique.

\section{Étude de la sensibilité de la solution}
Nous avons déflouté en boucle les différents exemples pour des valeurs de $\lambda$ différentes avec une limite de 3000 itérations. Le tableau suivant montre l'ensemble de nos résultats, pour les deux méthodes. \\

Pour la première image, on voit que l'erreur diminue constamment avec $\lambda$, en se stabilisant petit à petit ($\lambda$ tend en fait vers 0). \\
Pour la seconde image, l'erreur est minimale autour de $\lambda = 10^{-4}$ avec la méthode de descente de coordonnées, mais se stabilise après $\lambda = 5\cdot10^{-7}$ avec la méthode du gradient. \\
L'erreur de la dernière image diminue également progressivement jusqu'à se stabiliser après $\lambda = 10^{-7}$.

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{Méthode} & \multirow{2}{*}{$\lambda$} & \multicolumn{3}{|c|}{Erreur} \\\cline{3-5}
 & & Example0 & Example1 & Example2 \\
 \hline
\multirow{18}{*}{Descente de coordonnées} & 1,00E-02 & 47,14617916 & 10,1647664 & 30,72163516 \\
 & 5,00E-03 & 38,66568078 & 9,169684144 & 26,8902945 \\
 & 1,00E-03 & 21,3446921 & 7,167769831 & 18,05490584 \\
 & 5,00E-04 & 15,28981797 & 6,453585874 & 14,62302629 \\
 & 1,00E-04 & 5,446041966 & 6,048189766 & 7,727587915 \\
 & 5,00E-05 & 3,155836022 & 6,416526121 & 5,293370035 \\
 & 1,00E-05 & 1,250600831 & 9,522410563 & 1,868970892 \\
 & 5,00E-06 & 1,058235509 & 10,95138518 & 1,375975318 \\
 & 1,00E-06 & 0,934905051 & 12,60119727 & 1,050864016 \\
 & 5,00E-07 & 0,921463922 & 12,84824921 & 1,017104882 \\
 & 1,00E-07 & 0,911350647 & 13,04974679 & 0,990150346 \\
 & 5,00E-08 & 0,910149684 & 13,07534406 & 0,98675349 \\
 & 1,00E-08 & 0,909188837 & 13,09592037 & 0,984038304 \\
 & 5,00E-09 & 0,90907554 & 13,09849993 & 0,983700172 \\
 & 1,00E-09 & 0,908979452 & 13,10056444 & 0,983429913 \\
 & 5,00E-10 & 0,908967441 & 13,10082255 & 0,98339619 \\
 & 1,00E-10 & 0,908957833 & 13,10102905 & 0,98336921 \\
 & 5,00E-11 & 0,908956632 & 13,10105487 & 0,983365838 \\
\hline
\multirow{18}{*}{Gradient} & 1,00E-02 & 47,14785368 & 10,1657138 & 30,72596525 \\
 & 5,00E-03 & 38,66362675 & 9,169837985 & 26,89259855 \\
 & 1,00E-03 & 21,341731 & 7,163593487 & 18,05553437 \\
 & 5,00E-04 & 15,29204685 & 6,476905571 & 14,63874942 \\
 & 1,00E-04 & 5,835811798 & 5,465286598 & 8,818225907 \\
 & 5,00E-05 & 3,909591126 & 5,340744131 & 7,547420843 \\
 & 1,00E-05 & 2,24333251 & 5,2763741 & 6,386730072 \\
 & 5,00E-06 & 2,026906195 & 5,272683027 & 6,234823196 \\
 & 1,00E-06 & 1,84494184 & 5,271115461 & 6,11199903 \\
 & 5,00E-07 & 1,821932862 & 5,271049484 & 6,096575705 \\
 & 1,00E-07 & 1,803569205 & 5,271017599 & 6,084220985 \\
 & 5,00E-08 & 1,801278858 & 5,271013461 & 6,082674497 \\
 & 1,00E-08 & 1,799434918 & 5,271010282 & 6,08143701 \\
 & 5,00E-09 & 1,799204933 & 5,271009887 & 6,081282306 \\
 & 1,00E-09 & 1,799020708 & 5,271009571 & 6,081158539 \\
 & 5,00E-10 & 1,798997653 & 5,271009531 & 6,081143068 \\
 & 1,00E-10 & 1,798979228 & 5,271009499 & 6,081130692 \\
 & 5,00E-11 & 1,798976924 & 5,271009495 & 6,081129144 \\
\hline

\end{tabular}
\end{center}
\section {Conclusion}
Ce projet nous a permis d'appliquer les notions théoriques vues au cours et de les étendre à un domaine concret. Nous avons en effet adapté ces notions de base à un problème axé sur le monde numérique.\\
\\Le projet a été mené à bien. Nous sommes parvenus à déflouter l'ensemble des images fournies en implémentant la méthode du gradient et la méthode de descente de coordonnées, ce qui nous a par la suite permis d'en effectuer une comparaison.\\
Pour chaque méthode, nous avons ensuite pu établir des valeurs pour $\lambda$ telles que le pourcentage d'erreur soit minimal.

\end{document}