\documentclass[12pt, a4paper]{report}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{subsection}}
\usepackage[a4paper,left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}
\usepackage[fpms]{umons-coverpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{multirow}
\umonsAuthor{Sabrine \textsc{Riahi}\\Aymerick \textsc{Soyez}}
\usepackage[utf8]{inputenc}
\umonsTitle{Optimisation Non Linéaire}
\umonsSubtitle {\\ \textbf{Récupération d’une image floutée (deblurring)}}
\umonsSupervisor {Sous la direction de Monsieur Nicolas \textsc{Gillis} et Arnaud \textsc{Vandaele}}
\umonsDate {Décembre 2017}
\umonsDocumentType {Projet d'Optimisation}

\begin{document}
\umonsCoverPage
\tableofcontents
\clearpage
\section{Introduction}
Le problème posé est de déflouter une image dont chaque pixel a été remplacé par une combinaison linéaire des pixels voisins. La matrice de floutage utilisée est donnée. De plus, un bruit gaussien a été ajouté dans l'image.\\
L'objectif de ce projet est donc la résolution du problème suivant :\\
\[\underset{0 \leq x \leq 1}{\mathrm{min}} ||Ax - \tilde{x}||_2^2 + \lambda||x||_2^2\]
où :\\
$A$ est la matrice de floutage\\
$\tilde{x}$ est le vecteur de pixels flouté\\
$\lambda$ est un paramètre positif qui dépend du niveau de bruit\\

\section{Étude de la convexité du problème}
\noindent
Pour qu'un problème soit convexe, il faut que
\begin{itemize}
\item Le problème soit sous forme de minimisation,
\item Son domaine $ D $ soit convexe,
\item $\forall x,y \in D,\ \forall \lambda \in \left[0;1\right],\ f(\lambda x + (1 - \lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)$
\end{itemize}
\noindent
\linebreak
Le domaine est décrit par \( D = \left\{ x\ |\ c(x) \geq 0 \right\} \) et est convexe si $c(x)$ est concave.\\
Ici : \( D = \left\{ x\ |\ 0 \leq x \leq 1 \right\}\)\\
On a donc deux inégalités $x \geq 0$ et $1 - x \geq 0$. Ces deux fonctions étant linéaires, elles sont à la fois concaves et convexes, donc ces deux \og sous-domaines\fg\ sont convexes.\\
Puisque l'intersection de deux ensembles convexes est un ensemble convexe, l'ensemble $D$ est également convexe.\\

\noindent
La norme 2 est une fonction convexe.\\

\noindent
\textbf{Preuve}\\
En utilisant les propriétés de la norme euclidienne :\\
$\begin{array}{rcl}
f(\lambda x + (1-\lambda)y) & = & ||\ \lambda x + (1-\lambda)y\ ||\\
 & \leq & \lambda\ ||x|| + (1-\lambda)\ ||y||\\
 & = & \lambda f(x) + (1-\lambda)f(y)\\
\end{array}$

\noindent
\newline
On a donc bien une fonction convexe, et on peut conclure que le problème est convexe.\\


\section{Le problème admet-il un minimum global ?}
Le domaine du problème est compact (fermé et borné). De plus, la fonction est convexe sur ce domaine. On aura donc soit un ou plusieurs minima locaux à l'intérieur du domaine, soit un minimum local sur l'une de ses bornes.\\
Le problème étant convexe, tout minimum local est global.\\
On conclut ainsi que le problème admet un minimum global.\\

\section{Conditions d'optimalité}

\section{Méthode de descente de coordonnées}
La méthode de descente de coordonnées consiste à minimiser la fonction successivement sur chaque variable séparément en gardant les autres variables fixées.\\
\\Pour cela, on choisit un point de départ, qui sera, dans le cadre de ce projet, l'image floutée.\\
On calcule ensuite, pour chaque variable, la dérivée par rapport à celle-ci, que l'on égalise à zéro. La valeur obtenue remplacera la précédente pour les prochaines itérations. \\
\\On effectue ces opérations jusqu'à obtenir une convergence des valeurs obtenues pour chaque variable, c'est à dire jusqu'à ce que l'écart entre deux itérations soit inférieur à une précision fixée. \\
\\La fonction que nous devons minimiser est quadratique. Mettons-là sous la forme \[f(x) = \dfrac{1}{2}x^TQx + c^Tx + b\]

\begin{tabular}{rcl}
$||Ax - \tilde{x}||_2^2 + \lambda||x||_2^2$ & = & $(Ax - \tilde{x})^T(Ax - \tilde{x}) + \lambda x^Tx$ \\
 & = & $(Ax)^T(Ax) - \tilde{x}^T(Ax) - (Ax)^T\tilde{x} + \tilde{x}^T\tilde{x} + \lambda x^Tx$ \\
 & = & $x^TA^TAx - 2\tilde{x}^TAx + \tilde{x}^T\tilde{x} + x^T(\lambda I)x$ \\
 & = & $x^T(A^TA + \lambda I)x - 2 \tilde{x}^TAx + \tilde{x}^T\tilde{x}$ \\
\end{tabular}
\newline
\newline
\\
On retrouve bien la forme ci-dessus, avec
$\left\{\begin{array}{rcl}
Q & = & 2(A^TA + \lambda I) \\
c & = & -2A^T\tilde{x} \\
b & = & \tilde{x}^T\tilde{x} \\
\end{array}\right.$
\\
Pour calculer la formule de l'actualisation d'une variable, considérons une fonction quadratique en 3 dimensions : \\
\[f(x_1,x_2,x_3) = \dfrac{1}{2}
\left(\begin{array}{ccc}
x_1 & x_2 & x_3
\end{array}\right)
\left(\begin{array}{ccc}
q_{11} & q_{12} & q_{13} \\
q_{21} & q_{22} & q_{23} \\
q_{31} & q_{32} & q_{33}
\end{array}\right)
\left(\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}\right)
+ \left(\begin{array}{ccc}
c_1 & c_2 & c_3
\end{array}\right)
\left(\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}\right)\]

Si on veut actualiser $x_2$, on calcule la dérivée de $f$ en cette variable :
\[\dfrac{\delta f(x_1,x_2,x_3)}{\delta x_2} = q_{21}x_1 + q_{22}x_2 + q_{23}x_3 + c_2\]
La nouvelle valeur de $x_2$ est donc donnée par :
\(x_2 = \dfrac{-(q_{21}x_1 + q_{23}x_3 + c_2)}{q_{22}}\) \\
Pour une fonction à $n$ dimensions, en la variable $x_i$ :\\
\begin{center}
\begin{tabular}{rcl}
$x_i$ & = & $\dfrac{-\left(\sum\limits_{\underset{j \neq i}{j = 1}}^n q_{ij}x_j + c_i\right)}{q_{ii}}$ \\
 & = & $x_i - \dfrac{\sum\limits_{j = 1}^nq_{ij}x_j + c_i}{q_{ii}}$ \\
 & = & $x_i - \dfrac{\left[\nabla f\right]_i}{q_{ii}}$ \\
\end{tabular}
\end{center}

Pour respecter la contrainte $0 \leq x \leq 1$, il suffit d'utiliser \(\text{min}(\text{max}(..., 0), 1)\), où "..." est la formule d'actualisation ci-dessus. \\

Nous avons également utilisé un test d'arrêt qui prend en compte la norme de l'écart entre les deux dernières itérations ainsi que le nombre d'itérations. \\
L'implémentation sous Matlab se trouve en annexes.

\section{Méthode du gradient}
La méthode du gradient est similaire à la méthode de descente de coordonnées, en prenant comme direction de descente la direction de plus grande pente.\\
Afin de s'assurer que l'objectif diminue, on détermine un pas optimal pour chaque itération.\\
\\Une itération sera donnée par :
\begin{center}
$X_{k+1}=X_k - \alpha \nabla f(X_k)$
\end{center}

\section{Comparaison des méthodes}
Dans le tableau suivant reprenant l'ensemble des erreurs pour diverses valeurs de $\lambda$ (limité à 3000 itérations), on peut voir que la méthode du gradient est plus efficace pour l'Example1, tandis que la méthode de descente de coordonnées est plus efficace pour la première et la dernière image. \\

%La méthode de descente de coordonnées est en générale plus efficace (converge plus rapidement) pour des fonctions dont on peut séparer les différentes variables : $f(x_1,x_2) = g(x_1) + h(x_2)$, ce qui est le cas d'une fonction quadratique.

\section{Étude de la sensibilité de la solution}
Nous avons déflouté en boucle les différents exemples pour des valeurs de $\lambda$ différentes avec une limite de 3000 itérations. Le tableau suivant montre l'ensemble de nos résultats, pour les deux méthodes. \\

Pour la première image, on voit que l'erreur diminue constamment avec $\lambda$, en se stabilisant petit à petit ($\lambda$ tend en fait vers 0). \\
Pour la seconde image, l'erreur est minimale autour de $\lambda = 10^{-4}$ avec la méthode de descente de coordonnées, mais se stabilise après $\lambda = 5\cdot10^{-7}$ avec la méthode du gradient. \\
L'erreur de la dernière image diminue également progressivement jusqu'à se stabiliser après $\lambda = 10^{-7}$.

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{Méthode} & \multirow{2}{*}{$\lambda$} & \multicolumn{3}{|c|}{Erreur} \\\cline{3-5}
 & & Example0 & Example1 & Example2 \\
 \hline
\multirow{18}{*}{Descente de coordonnées} & 1,00E-02 & 47,14617916 & 10,1647664 & 30,72163516 \\
 & 5,00E-03 & 38,66568078 & 9,169684144 & 26,8902945 \\
 & 1,00E-03 & 21,3446921 & 7,167769831 & 18,05490584 \\
 & 5,00E-04 & 15,28981797 & 6,453585874 & 14,62302629 \\
 & 1,00E-04 & 5,446041966 & 6,048189766 & 7,727587915 \\
 & 5,00E-05 & 3,155836022 & 6,416526121 & 5,293370035 \\
 & 1,00E-05 & 1,250600831 & 9,522410563 & 1,868970892 \\
 & 5,00E-06 & 1,058235509 & 10,95138518 & 1,375975318 \\
 & 1,00E-06 & 0,934905051 & 12,60119727 & 1,050864016 \\
 & 5,00E-07 & 0,921463922 & 12,84824921 & 1,017104882 \\
 & 1,00E-07 & 0,911350647 & 13,04974679 & 0,990150346 \\
 & 5,00E-08 & 0,910149684 & 13,07534406 & 0,98675349 \\
 & 1,00E-08 & 0,909188837 & 13,09592037 & 0,984038304 \\
 & 5,00E-09 & 0,90907554 & 13,09849993 & 0,983700172 \\
 & 1,00E-09 & 0,908979452 & 13,10056444 & 0,983429913 \\
 & 5,00E-10 & 0,908967441 & 13,10082255 & 0,98339619 \\
 & 1,00E-10 & 0,908957833 & 13,10102905 & 0,98336921 \\
 & 5,00E-11 & 0,908956632 & 13,10105487 & 0,983365838 \\
\hline
\multirow{18}{*}{Gradient} & 1,00E-02 & 47,14785368 & 10,1657138 & 30,72596525 \\
 & 5,00E-03 & 38,66362675 & 9,169837985 & 26,89259855 \\
 & 1,00E-03 & 21,341731 & 7,163593487 & 18,05553437 \\
 & 5,00E-04 & 15,29204685 & 6,476905571 & 14,63874942 \\
 & 1,00E-04 & 5,835811798 & 5,465286598 & 8,818225907 \\
 & 5,00E-05 & 3,909591126 & 5,340744131 & 7,547420843 \\
 & 1,00E-05 & 2,24333251 & 5,2763741 & 6,386730072 \\
 & 5,00E-06 & 2,026906195 & 5,272683027 & 6,234823196 \\
 & 1,00E-06 & 1,84494184 & 5,271115461 & 6,11199903 \\
 & 5,00E-07 & 1,821932862 & 5,271049484 & 6,096575705 \\
 & 1,00E-07 & 1,803569205 & 5,271017599 & 6,084220985 \\
 & 5,00E-08 & 1,801278858 & 5,271013461 & 6,082674497 \\
 & 1,00E-08 & 1,799434918 & 5,271010282 & 6,08143701 \\
 & 5,00E-09 & 1,799204933 & 5,271009887 & 6,081282306 \\
 & 1,00E-09 & 1,799020708 & 5,271009571 & 6,081158539 \\
 & 5,00E-10 & 1,798997653 & 5,271009531 & 6,081143068 \\
 & 1,00E-10 & 1,798979228 & 5,271009499 & 6,081130692 \\
 & 5,00E-11 & 1,798976924 & 5,271009495 & 6,081129144 \\
\hline

\end{tabular}
\end{center}


\end{document}